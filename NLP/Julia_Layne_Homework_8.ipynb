{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Homework 8\n",
    "## Sentiment Analysis\n",
    "### Julia Layne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vocabulary-based sentiment analysis of the movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Cluster  0 ----\n",
      "Average:  3.2083333333333335\n",
      "Median:  2.3125\n",
      "High:  11.25\n",
      "Low:  -0.125\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Mostly negative MidSized reviews. Even positives point out negatives\n",
      "---- Cluster  1 ----\n",
      "Average:  0.49375\n",
      "Median:  0.5\n",
      "High:  5.75\n",
      "Low:  -3.5\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Mostly negative and Luke-warm reviews\n",
      "---- Cluster  2 ----\n",
      "Average:  0.90625\n",
      "Median:  0.1875\n",
      "High:  4.0\n",
      "Low:  -0.75\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Short reviews, positive for the dark comedy and negative on the regular comedies\n",
      "---- Cluster  3 ----\n",
      "Average:  1.2022068965517243\n",
      "Median:  1.25\n",
      "High:  8.25\n",
      "Low:  -6.375\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Mostly Positive, even the negative reviews are softer: 'The premise is cute and the movie seemed like it was going places'\n",
      "---- Cluster  4 ----\n",
      "Average:  0.25\n",
      "Median:  -0.0625\n",
      "High:  5.0\n",
      "Low:  -3.5\n",
      "Overall Negative\n",
      "Old Description:\n",
      " Mostly Positive, Longer reviews\n",
      "---- Cluster  5 ----\n",
      "Average:  -1.375\n",
      "Median:  -1.375\n",
      "High:  -0.375\n",
      "Low:  -2.375\n",
      "Overall Negative\n",
      "Old Description:\n",
      " Two short reviews for I Blame Society - Comedy Horror\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"parsed_reviews_sentiment.csv\")\n",
    "for i in range(6):\n",
    "    print('---- Cluster ',i,'----')\n",
    "    filtered = reviews['cluster']==i\n",
    "    review_filtered = reviews.copy()\n",
    "    review_filtered.where(filtered, inplace = True)\n",
    "    review_filtered = review_filtered.dropna()\n",
    "    #average, median, high, and low\n",
    "    print('Average: ',review_filtered['SENTIMENT'].mean())\n",
    "    print('Median: ',review_filtered['SENTIMENT'].median())\n",
    "    print('High: ',review_filtered['SENTIMENT'].max())\n",
    "    print('Low: ',review_filtered['SENTIMENT'].min())\n",
    "    if review_filtered['SENTIMENT'].median() >0:\n",
    "        print('Overall Positive')\n",
    "    else:\n",
    "        print('Overall Negative')\n",
    "    print(\"Old Description:\\n\",oldDescription[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tIn Python, load one of the sentiment vocabularies referenced in the textbook, and run the sentiment analyzer as explained in the corresponding reference. Add words to the sentiment vocabulary, if you think you need to, to better fit your particular text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"parsed_reviews_clustered6.csv\")\n",
    "reviews[\"SENTIMENT\"] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sourced from this article with some changes to get the swap from pennbank \n",
    "#https://nlpforhackers.io/sentiment-analysis-intro/\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    if tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    if tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    if tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    " \n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    return text\n",
    " \n",
    " \n",
    "def swn_polarity(text):\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    text = clean_text(text)\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    for raw_sentence in raw_sentences:\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    " \n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    "    #return the overall sentiment instead of the 0/1 for positive negative to allow for handling later\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.75\n"
     ]
    }
   ],
   "source": [
    "# Since we're shuffling, you'll get diffrent results\n",
    "print(swn_polarity(reviews[\"CONTENT\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-b93bb20e8b2d>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews[\"SENTIMENT\"][i] = sentiment\n"
     ]
    }
   ],
   "source": [
    "for i in reviews.index:\n",
    "    text = reviews[\"CONTENT\"][i]\n",
    "    sentiment = swn_polarity(text)\n",
    "    #print(sentiment)\n",
    "    reviews[\"SENTIMENT\"][i] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(r'parsed_reviews_sentiment.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>SOUP</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>NP_CHUNK</th>\n",
       "      <th>cluster</th>\n",
       "      <th>SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.imdb.com/review/rw6540019/?ref_=tt...</td>\n",
       "      <td>\\n&lt;!DOCTYPE html&gt;\\n\\n&lt;html xmlns:fb=\"http://ww...</td>\n",
       "      <td>It might have been an error on Disney's part t...</td>\n",
       "      <td>['It', 'an', 'error', 'Disney', 'part', 'episo...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.imdb.com/review/rw6562703/?ref_=tt...</td>\n",
       "      <td>\\n&lt;!DOCTYPE html&gt;\\n\\n&lt;html xmlns:fb=\"http://ww...</td>\n",
       "      <td>Writing this with bated breath waiting on epis...</td>\n",
       "      <td>['bated', 'breath', 'episode', 'The', 'whole',...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.imdb.com/review/rw6650913/?ref_=tt...</td>\n",
       "      <td>\\n&lt;!DOCTYPE html&gt;\\n\\n&lt;html xmlns:fb=\"http://ww...</td>\n",
       "      <td>One might wonder that the above adjectives do ...</td>\n",
       "      <td>['the', 'above', 'adjectives', 'the', 'Marvel'...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.imdb.com/review/rw6603814/?ref_=tt...</td>\n",
       "      <td>\\n&lt;!DOCTYPE html&gt;\\n\\n&lt;html xmlns:fb=\"http://ww...</td>\n",
       "      <td>I can't speak for the reviews that bash this s...</td>\n",
       "      <td>['I', 'the', 'reviews', 'this', 'show', 'three...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.imdb.com/review/rw6479388/?ref_=tt...</td>\n",
       "      <td>\\n&lt;!DOCTYPE html&gt;\\n\\n&lt;html xmlns:fb=\"http://ww...</td>\n",
       "      <td>What a waste $25 Million Dollars A episode don...</td>\n",
       "      <td>['a', 'waste', '25', 'Million', 'Dollars', 'A'...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://www.imdb.com/review/rw6540019/?ref_=tt...   \n",
       "1  https://www.imdb.com/review/rw6562703/?ref_=tt...   \n",
       "2  https://www.imdb.com/review/rw6650913/?ref_=tt...   \n",
       "3  https://www.imdb.com/review/rw6603814/?ref_=tt...   \n",
       "4  https://www.imdb.com/review/rw6479388/?ref_=tt...   \n",
       "\n",
       "                                                SOUP  \\\n",
       "0  \\n<!DOCTYPE html>\\n\\n<html xmlns:fb=\"http://ww...   \n",
       "1  \\n<!DOCTYPE html>\\n\\n<html xmlns:fb=\"http://ww...   \n",
       "2  \\n<!DOCTYPE html>\\n\\n<html xmlns:fb=\"http://ww...   \n",
       "3  \\n<!DOCTYPE html>\\n\\n<html xmlns:fb=\"http://ww...   \n",
       "4  \\n<!DOCTYPE html>\\n\\n<html xmlns:fb=\"http://ww...   \n",
       "\n",
       "                                             CONTENT  \\\n",
       "0  It might have been an error on Disney's part t...   \n",
       "1  Writing this with bated breath waiting on epis...   \n",
       "2  One might wonder that the above adjectives do ...   \n",
       "3  I can't speak for the reviews that bash this s...   \n",
       "4  What a waste $25 Million Dollars A episode don...   \n",
       "\n",
       "                                            NP_CHUNK  cluster SENTIMENT  \n",
       "0  ['It', 'an', 'error', 'Disney', 'part', 'episo...        2     -0.75  \n",
       "1  ['bated', 'breath', 'episode', 'The', 'whole',...        0         2  \n",
       "2  ['the', 'above', 'adjectives', 'the', 'Marvel'...        1      -3.5  \n",
       "3  ['I', 'the', 'reviews', 'this', 'show', 'three...        0     2.625  \n",
       "4  ['a', 'waste', '25', 'Million', 'Dollars', 'A'...        0    -0.125  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\t For each of the clusters you created in homework 7, compute the average, median, high, and low sentiment scores for each cluster. Explain whether you think this reveals anything interesting about the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldDescription = [\n",
    "    \"Mostly negative MidSized reviews. Even positives point out negatives\",\n",
    "    \"Mostly negative and Luke-warm reviews\",\n",
    "    \"Short reviews, positive for the dark comedy and negative on the regular comedies\",\n",
    "    \"Mostly Positive, even the negative reviews are softer: 'The premise is cute and the movie seemed like it was going places'\",\n",
    "    \"Mostly Positive, Longer reviews\",\n",
    "    \"Two short reviews for I Blame Society - Comedy Horror\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Cluster  0 ----\n",
      "Average:  3.2083333333333335\n",
      "Median:  2.3125\n",
      "High:  11.25\n",
      "Low:  -0.125\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Mostly negative MidSized reviews. Even positives point out negatives\n",
      "---- Cluster  1 ----\n",
      "Average:  0.49375\n",
      "Median:  0.5\n",
      "High:  5.75\n",
      "Low:  -3.5\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Mostly negative and Luke-warm reviews\n",
      "---- Cluster  2 ----\n",
      "Average:  0.90625\n",
      "Median:  0.1875\n",
      "High:  4.0\n",
      "Low:  -0.75\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Short reviews, positive for the dark comedy and negative on the regular comedies\n",
      "---- Cluster  3 ----\n",
      "Average:  1.2022068965517243\n",
      "Median:  1.25\n",
      "High:  8.25\n",
      "Low:  -6.375\n",
      "Overall Positive\n",
      "Old Description:\n",
      " Mostly Positive, even the negative reviews are softer: 'The premise is cute and the movie seemed like it was going places'\n",
      "---- Cluster  4 ----\n",
      "Average:  0.25\n",
      "Median:  -0.0625\n",
      "High:  5.0\n",
      "Low:  -3.5\n",
      "Overall Negative\n",
      "Old Description:\n",
      " Mostly Positive, Longer reviews\n",
      "---- Cluster  5 ----\n",
      "Average:  -1.375\n",
      "Median:  -1.375\n",
      "High:  -0.375\n",
      "Low:  -2.375\n",
      "Overall Negative\n",
      "Old Description:\n",
      " Two short reviews for I Blame Society - Comedy Horror\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print('---- Cluster ',i,'----')\n",
    "    filtered = reviews['cluster']==i\n",
    "    review_filtered = reviews.copy()\n",
    "    review_filtered.where(filtered, inplace = True)\n",
    "    review_filtered = review_filtered.dropna()\n",
    "    #average, median, high, and low\n",
    "    print('Average: ',review_filtered['SENTIMENT'].mean())\n",
    "    print('Median: ',review_filtered['SENTIMENT'].median())\n",
    "    print('High: ',review_filtered['SENTIMENT'].max())\n",
    "    print('Low: ',review_filtered['SENTIMENT'].min())\n",
    "    if review_filtered['SENTIMENT'].median() >0:\n",
    "        print('Overall Positive')\n",
    "    else:\n",
    "        print('Overall Negative')\n",
    "    print(\"Old Description:\\n\",oldDescription[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tFor extra credit, analyze sentiment of chunks as follows:\n",
    " - a.\tTake the chunks from homework 5, and in Python, run each chunk individually through your sentiment analyzer that you used in question 1. If the chunk registers a nonneutral sentiment, save it in a tabular format (the chunk, the sentiment score).\n",
    " - b.\tNow sort the table twice, once to show the highest negative-sentiment-scoring chunks at the top and again to show the highest positive-sentiment-scoring chunks at the top. Examine the upper portions of both sorted lists, to identify any trends, and explain what you see. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit all of your inputs and outputs and your code for this assignment, along with a brief written explanation of your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
