{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.metrics import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tCompare your given name with your nickname (if you don’t have a nickname, invent one for this assignment) by answering the following questions:\n",
    "\n",
    "a.\tWhat is the edit distance between your nickname and your given name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The edit distance between julia and jules is 2\n"
     ]
    }
   ],
   "source": [
    "name = \"julia\"\n",
    "nickname = 'jules'\n",
    "print(\"The edit distance between \" +name+ \" and \" + nickname\n",
    "      + \" is \" +str(distance.edit_distance(name,nickname)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " b. What is the percentage string match between your nickname and your given name? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "print(Levenshtein.ratio(name, nickname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Levenshtein distance to get the matching characters, we get a 60% match. This makes sense because both name and nickname are 5 letters and 3 out of the 5 letters match between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tFind a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think you friend either was or was not able to guess the book from hearing the list of words. \n",
    "\n",
    ">\t“On the White House roof, tucked into a corner of the Promenade, there’s a bit of loos paneling right on the edge of the Solarium. If you tap it just right, you can peel it back enough to find a message etched underneath, with the tip of a key or maybe a stolen West Wing letter opener.” -Red, White, and Royal Blue Casey McQuinton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"On the White House roof, tucked into a corner of the Promenade, there’s a bit of loose paneling right on the edge of the Solarium. If you tap it just right, you can peel it back enough to find a message etched underneath, with the tip of a key or maybe a stolen West Wing letter opener.\"\n",
    "quote_array =  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "eng_stop = stopwords.words('english')\n",
    "#print(eng_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On White House roof , tucked corner Promenade , ’ bit loose paneling right edge Solarium . If tap right , peel back enough find message etched underneath , tip key maybe stolen West Wing letter opener .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in word_tokenize(quote):\n",
    "    if word in eng_stop:\n",
    "        None#print(word) \n",
    "    else:\n",
    "        quote_array.append(word)\n",
    "non_stop_quote = \" \" \n",
    "non_stop_quote.join(quote_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My roommate was able to get this on the second try. Given the proper nouns like 'White House' and 'West Wing', she was able to narrow it down quickly to which book it came from. The missing words did not take away from the meaning, as we would expect with stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tRun one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Stemmed: \n",
      "On \t On\n",
      "White \t white\n",
      "House \t hous\n",
      "roof \t roof\n",
      ", \t ,\n",
      "tucked \t tuck\n",
      "corner \t corner\n",
      "Promenade \t promenad\n",
      ", \t ,\n",
      "’ \t ’\n",
      "bit \t bit\n",
      "loose \t loos\n",
      "paneling \t panel\n",
      "right \t right\n",
      "edge \t edg\n",
      "Solarium \t solarium\n",
      ". \t .\n",
      "If \t If\n",
      "tap \t tap\n",
      "right \t right\n",
      ", \t ,\n",
      "peel \t peel\n",
      "back \t back\n",
      "enough \t enough\n",
      "find \t find\n",
      "message \t messag\n",
      "etched \t etch\n",
      "underneath \t underneath\n",
      ", \t ,\n",
      "tip \t tip\n",
      "key \t key\n",
      "maybe \t mayb\n",
      "stolen \t stolen\n",
      "West \t west\n",
      "Wing \t wing\n",
      "letter \t letter\n",
      "opener \t open\n",
      ". \t .\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "#nltk.download('punkt')\n",
    "stemmed_array = []\n",
    "for word in quote_array:\n",
    "    stemmed_array.append(ps.stem(word))\n",
    "    \n",
    "print(\"Original: Stemmed: \")\n",
    "for i in range(len(quote_array)):\n",
    "    print(quote_array[i] +\" \\t \" + stemmed_array[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% of the words in these sentences were valid morphological roots. The sentences used in the book did not have any tricky words. This straightforward stemming was able to easily find the correct roots of each word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
